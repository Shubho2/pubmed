39. Acad Radiol. 2018 Jun 19. pii: S1076-6332(18)30259-9. doi:10.1016/j.acra.2018.05.017. [Epub ahead of print]Interobserver Variation in Response Evaluation Criteria in Solid Tumors 1.1.Karmakar A(1), Kumtakar A(2), Sehgal H(3), Kumar S(4), Kalyanpur A(5).Author information: (1)Image Core Labs division, Teleradiology Solutions Pvt. Ltd., Plot # 7G, OppGraphite India Whitefield, Bengaluru, Karnataka 560048, India. Electronicaddress: Arunabha.Karmakar@gmail.com.(2)Image Core Labs division, Teleradiology Solutions Pvt. Ltd., Plot # 7G, OppGraphite India Whitefield, Bengaluru, Karnataka 560048, India. Electronicaddress: Apeksha7689@gmail.com.(3)Image Core Labs division, Teleradiology Solutions Pvt. Ltd., Plot # 7G, OppGraphite India Whitefield, Bengaluru, Karnataka 560048, India. Electronicaddress: HIMANSHU.s2208@gmail.com.(4)Teleradiology Solutions Pvt. Ltd., Bengaluru, Karnataka, India. Electronicaddress: Major.Savith@gmail.com.(5)Teleradiology Solutions Pvt. Ltd., Bengaluru, Karnataka, India. Electronicaddress: Arjun.Kalyanpur@telradsol.com.PURPOSE: Response Evaluation Criteria in Solid Tumors (RECIST 1.1) is the goldstandard for imaging response evaluation in cancer trials. We sought to evaluate consistency of applying RECIST 1.1 between 2 conventionally trained radiologists,designated as A and B; identify reasons for variation; and reconcile thesedifferences for future studies.METHODS: The study was approved as an institutional quality check exercise. Sinceno identifiable patient data was collected or used, a waiver of informed consent was granted. Imaging case report forms of a concluded multicentric breast cancer trial were retrospectively reviewed. Cohen's kappa was used to rate interobserveragreement in Response Evaluation Data (target response, nontarget response, newlesions, overall response). Significant variations were reassessed by a seniorradiologist to extrapolate reasons for disagreement. Methods to improve agreementwere similarly ascertained.RESULTS: Sixty one cases with total of 82 data-pairs were evaluated (35data-pairs in visit 5, 47 in visit 9). Both radiologists showed moderateagreement in target response (n = 82; ĸ = 0.477; 95% confidence interval [CI]:0.314-0.640-), nontarget response (n = 82; ĸ = 0.578; 95% CI: 0.213-0.944) andoverall response evaluation in both visits (n = 82; ĸ = 0.510; 95% CI:0.344-0.676). Further assessment demonstrated "Prevalence effect" of Kappa insome cases which led to underestimation of agreement. Percent agreement ofoverall response was 74.39% while percent variation was 25.6%. Differences ininterpreting RECIST 1.1 and in radiological image interpretation were the primarysources of variation. The commonest overall response was "Partial Response" (Rad A:45/82; Rad B:63/82).CONCLUSION: Inspite of moderate interobserver agreement, qualitativeinterpretation differences in some cases increased interobserver variability.Protocols such as Adjudication, to reduce easily avoidable inconsistencies are orshould be a part of the Standard Operating Procedure in imaging institutions.Based on our findings, a standard checklist has been developed to help reduce theinterpretation error-margin for future studies. Such check-lists may improveinterobserver agreement in the preadjudication phase thereby improving quality ofresults and reducing adjudication per case ratio.CLINICAL RELEVANCE: Improving data reliability when using RECIST 1.1 will reflectin better cancer clinical trial outcomes. A checklist can be of use to imagingcenters to assess and improve their own processes.Copyright © 2018. Published by Elsevier Inc.DOI: 10.1016/j.acra.2018.05.017 PMID: 29934024 